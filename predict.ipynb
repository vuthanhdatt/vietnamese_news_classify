{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from underthesea import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance predict class\n",
    "class Predict(object):\n",
    "  def __init__(self, model, device):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "  def predict(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = F.softmax(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, masks, targets):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = torch.tensor(self.ids[index], dtype=torch.long)\n",
    "        masks = torch.tensor(self.masks[index], dtype=torch.long)\n",
    "        targets = torch.FloatTensor(self.targets[index])\n",
    "        return ids, masks, targets\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove words in paranthesis\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[^A-Za-z0-9wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]+',' ',text)# remove non alphanumeric chars\n",
    "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "   \n",
    "    return  word_tokenize(text, format='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def encode(self, y):\n",
    "        y_one_hot = np.zeros((len(y), len(self.class_to_index)), dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            y_one_hot[i][self.class_to_index[item]] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            index = np.where(item == 1)[0][0]\n",
    "            classes.append(self.index_to_class[index])\n",
    "        return classes\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\", encoding='utf-8') as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBert(nn.Module):\n",
    "    def __init__(self, phobert, dropout_p, embedding_dim, num_classes):\n",
    "        super(PhoBert, self).__init__()\n",
    "        self.phobert = phobert\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ids, masks = inputs\n",
    "        seq, pool = self.phobert(input_ids=ids, attention_mask=masks)\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_distribution(y_prob, classes):\n",
    "    \"\"\"Create a dict of class probabilities from an array.\"\"\"\n",
    "    results = {}\n",
    "    for i, class_ in enumerate(classes):\n",
    "        results[class_] = np.float64(y_prob[i])\n",
    "    sorted_results = {k: v for k, v in sorted(\n",
    "        results.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading nessessary models for processing and predicting\n",
    "le = LabelEncoder().load(\"model/label_encoder.json\")\n",
    "device = torch.device(\"cpu\")\n",
    "dropout_p = 0.5\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\", return_dict = False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", return_dict = False)\n",
    "embedding_dim = phobert.config.hidden_size\n",
    "model = PhoBert(\n",
    "phobert=phobert, dropout_p=dropout_p,\n",
    "embedding_dim=embedding_dim, num_classes=len(le))\n",
    "\n",
    "model.load_state_dict(torch.load(\"model/model.pt\", map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "\n",
    "predict_model = Predict(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "    X = preprocess(text)\n",
    "    encoded_input = tokenizer(X, return_tensors=\"pt\", padding=True).to(torch.device(\"cpu\"))\n",
    "    ids = encoded_input[\"input_ids\"]\n",
    "    masks = encoded_input[\"attention_mask\"]\n",
    "    y_filler = le.encode([le.classes[0]]*len(ids))\n",
    "    dataset = DataLoader(ids=ids, masks=masks, targets=y_filler)\n",
    "    dataloader = dataset.create_dataloader(batch_size=128)\n",
    "    y_prob = predict_model.predict(dataloader)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    result= le.index_to_class[y_pred[0]]\n",
    "    return result, y_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_distribution(y_prob, classes):\n",
    "    \"\"\"Create a dict of class probabilities from an array.\"\"\"\n",
    "    results = {}\n",
    "    for i, class_ in enumerate(classes):\n",
    "        results[class_] = np.float64(y_prob[i])\n",
    "    sorted_results = {k: v for k, v in sorted(\n",
    "        results.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinh doanh\n",
      "{'Kinh doanh': 0.9999986886978149, 'Thế giới': 3.819549760919472e-07, 'Thời sự': 3.720343499935552e-07, 'Thể thao': 3.132254846605065e-07, 'Sức khỏe': 2.881962757328438e-07}\n"
     ]
    }
   ],
   "source": [
    "label = predict_label('Giá dầu, chứng khoán thế giới đồng loạt giảm điểm')[0]\n",
    "probability = get_probability_distribution(predict_label('Giá dầu, chứng khoán thế giới đồng loạt giảm điểm')[1], le.classes)\n",
    "print(label)\n",
    "print(probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96bd5342b10575b808703df534a3a091192600cf2edaf696469e131f254440a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
